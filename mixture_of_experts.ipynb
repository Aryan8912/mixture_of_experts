{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMllPhnmFfBwaiR+7pBb20t",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Aryan8912/mixture_of_experts/blob/main/mixture_of_experts.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PKPZU-2sA95x"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import math\n",
        "from inspect import isfunction\n",
        "\n",
        "# constants\n",
        "\n",
        "MIN_EXPERT_CAPACITY = 4\n",
        "\n",
        "# helper functions\n",
        "\n",
        "def default(val, default_val):\n",
        "    default_val = default_val() if isfunction(default_val) else default_val\n",
        "    return val if val is not None else default_val\n",
        "\n",
        "def cast_tuple(el):\n",
        "    return el if isinstance(el, tuple) else (el,)\n",
        "\n",
        "# tensor related helper functions\n",
        "\n",
        "def top1(t):\n",
        "    values, index = t.topk(k=1, dim=-1)\n",
        "    values, index = map(lambda x: x.squeeze(dim=-1), (values, index))\n",
        "    return values, index\n",
        "\n",
        "def cumsum_exclusive(t, dim=-1):\n",
        "    num_dims = len(t.shape)\n",
        "    num_pad_dims = - dim - 1\n",
        "    pre_padding = (0, 0) * num_pad_dims\n",
        "    pre_slice   = (slice(None),) * num_pad_dims\n",
        "    padded_t = F.pad(t, (*pre_padding, 1, 0)).cumsum(dim=dim)\n",
        "    return padded_t[(..., slice(None, -1), *pre_slice)]\n",
        "\n",
        "# pytorch one hot throws an error if there are out of bound indices.\n",
        "# tensorflow, in contrast, does not throw an error\n",
        "def safe_one_hot(indexes, max_length):\n",
        "    max_index = indexes.max() + 1\n",
        "    return F.one_hot(indexes, max(max_index + 1, max_length))[..., :max_length]\n",
        "\n",
        "def init_(t):\n",
        "    dim = t.shape[-1]\n",
        "    std = 1 / math.sqrt(dim)\n",
        "    return t.uniform_(-std, std)\n",
        "\n",
        "# activations\n",
        "\n",
        "class GELU_(nn.Module):\n",
        "    def forward(self, x):\n",
        "        return 0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\n",
        "\n",
        "GELU = nn.GELU if hasattr(nn, 'GELU') else GELU_\n",
        "\n",
        "# expert class\n",
        "\n",
        "class Experts(nn.Module):\n",
        "    def __init__(self,\n",
        "        dim,\n",
        "        num_experts = 16,\n",
        "        hidden_dim = None,\n",
        "        activation = GELU):\n",
        "        super().__init__()\n",
        "\n",
        "        hidden_dim = default(hidden_dim, dim * 4)\n",
        "        num_experts = cast_tuple(num_experts)\n",
        "\n",
        "        w1 = torch.zeros(*num_experts, dim, hidden_dim)\n",
        "        w2 = torch.zeros(*num_experts, hidden_dim, dim)\n",
        "\n",
        "        w1 = init_(w1)\n",
        "        w2 = init_(w2)\n",
        "\n",
        "        self.w1 = nn.Parameter(w1)\n",
        "        self.w2 = nn.Parameter(w2)\n",
        "        self.act = activation()\n",
        "\n",
        "    def forward(self, x):\n",
        "        hidden = torch.einsum('...nd,...dh->...nh', x, self.w1)\n",
        "        hidden = self.act(hidden)\n",
        "        out    = torch.einsum('...nh,...hd->...nd', hidden, self.w2)\n",
        "        return out\n",
        "\n",
        "# the below code is almost all transcribed from the official tensorflow version, from which the papers are written\n",
        "# https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/models/research/moe.py\n",
        "\n",
        "# gating network\n",
        "\n",
        "class Top2Gating(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim,\n",
        "        num_gates,\n",
        "        eps = 1e-9,\n",
        "        outer_expert_dims = tuple(),\n",
        "        second_policy_train = 'random',\n",
        "        second_policy_eval = 'random',\n",
        "        second_threshold_train = 0.2,\n",
        "        second_threshold_eval = 0.2,\n",
        "        capacity_factor_train = 1.25,\n",
        "        capacity_factor_eval = 2.):\n",
        "        super().__init__()\n",
        "\n",
        "        self.eps = eps\n",
        "        self.num_gates = num_gates\n",
        "        self.w_gating = nn.Parameter(torch.randn(*outer_expert_dims, dim, num_gates))\n",
        "\n",
        "        self.second_policy_train = second_policy_train\n",
        "        self.second_policy_eval = second_policy_eval\n",
        "        self.second_threshold_train = second_threshold_train\n",
        "        self.second_threshold_eval = second_threshold_eval\n",
        "        self.capacity_factor_train = capacity_factor_train\n",
        "        self.capacity_factor_eval = capacity_factor_eval\n",
        "\n",
        "    def forward(self, x, importance = None):\n",
        "        *_, b, group_size, dim = x.shape\n",
        "        num_gates = self.num_gates\n",
        "\n",
        "        if self.training:\n",
        "            policy = self.second_policy_train\n",
        "            threshold = self.second_threshold_train\n",
        "            capacity_factor = self.capacity_factor_train\n",
        "        else:\n",
        "            policy = self.second_policy_eval\n",
        "            threshold = self.second_threshold_eval\n",
        "            capacity_factor = self.capacity_factor_eval\n",
        "\n",
        "        raw_gates = torch.einsum('...bnd,...de->...bne', x, self.w_gating)\n",
        "        raw_gates = raw_gates.softmax(dim=-1)\n",
        "\n",
        "        # FIND TOP 2 EXPERTS PER POSITON\n",
        "        # Find the top expert for each position. shape=[batch, group]\n",
        "\n",
        "        gate_1, index_1 = top1(raw_gates)\n",
        "        mask_1 = F.one_hot(index_1, num_gates).float()\n",
        "        density_1_proxy = raw_gates\n",
        "\n",
        "        if importance is not None:\n",
        "            equals_one_mask = (importance == 1.).float()\n",
        "            mask_1 *= equals_one_mask[..., None]\n",
        "            gate_1 *= equals_one_mask\n",
        "            density_1_proxy = density_1_proxy * equals_one_mask[..., None]\n",
        "            del equals_one_mask\n",
        "\n",
        "        gates_without_top_1 = raw_gates * (1. - mask_1)\n",
        "\n",
        "        gate_2, index_2 = top1(gates_without_top_1)\n",
        "        mask_2 = F.one_hot(index_2, num_gates).float()\n",
        "\n",
        "        if importance is not None:\n",
        "            greater_zero_mask = (importance > 0.).float()\n",
        "            mask_2 *= greater_zero_mask[..., None]\n",
        "            del greater_zero_mask\n",
        "\n",
        "        # normalize top2 gate scores\n",
        "        denom = gate_1 + gate_2 + self.eps\n",
        "        gate_1 /= denom\n",
        "        gate_2 /= denom\n",
        "\n",
        "        # BALANCING LOSSES\n",
        "        # shape = [batch, experts]\n",
        "        # We want to equalize the fraction of the batch assigned to each expert\n",
        "        density_1 = mask_1.mean(dim=-2)\n",
        "        # Something continuous that is correlated with what we want to equalize.\n",
        "        density_1_proxy = density_1_proxy.mean(dim=-2)\n",
        "        loss = (density_1_proxy * density_1).mean() * float(num_gates ** 2)\n",
        "\n",
        "        # Depending on the policy in the hparams, we may drop out some of the\n",
        "        # second-place experts.\n",
        "        if policy == \"all\":\n",
        "            pass\n",
        "        elif policy == \"none\":\n",
        "            mask_2 = torch.zeros_like(mask_2)\n",
        "        elif policy == \"threshold\":\n",
        "            mask_2 *= (gate_2 > threshold).float()\n",
        "        elif policy == \"random\":\n",
        "            probs = torch.zeros_like(gate_2).uniform_(0., 1.)\n",
        "            mask_2 *= (probs < (gate_2 / max(threshold, self.eps))).float().unsqueeze(-1)\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown policy {policy}\")\n",
        "\n",
        "        # Each sequence sends (at most?) expert_capacity positions to each expert.\n",
        "        # Static expert_capacity dimension is needed for expert batch sizes\n",
        "        expert_capacity = min(group_size, int((group_size * capacity_factor) / num_gates))\n",
        "        expert_capacity = max(expert_capacity, MIN_EXPERT_CAPACITY)\n",
        "        expert_capacity_f = float(expert_capacity)\n",
        "\n",
        "        # COMPUTE ASSIGNMENT TO EXPERTS\n",
        "        # [batch, group, experts]\n",
        "        # This is the position within the expert's mini-batch for this sequence\n",
        "        position_in_expert_1 = cumsum_exclusive(mask_1, dim=-2) * mask_1\n",
        "        # Remove the elements that don't fit. [batch, group, experts]\n",
        "        mask_1 *= (position_in_expert_1 < expert_capacity_f).float()\n",
        "        # [batch, experts]\n",
        "        # How many examples in this sequence go to this expert\n",
        "        mask_1_count = mask_1.sum(dim=-2, keepdim=True)\n",
        "        # [batch, group] - mostly ones, but zeros where something didn't fit\n",
        "        mask_1_flat = mask_1.sum(dim=-1)\n",
        "        # [batch, group]\n",
        "        position_in_expert_1 = position_in_expert_1.sum(dim=-1)\n",
        "        # Weight assigned to first expert.  [batch, group]\n",
        "        gate_1 *= mask_1_flat\n",
        "\n",
        "        position_in_expert_2 = cumsum_exclusive(mask_2, dim=-2) + mask_1_count\n",
        "        position_in_expert_2 *= mask_2\n",
        "        mask_2 *= (position_in_expert_2 < expert_capacity_f).float()\n",
        "        mask_2_flat = mask_2.sum(dim=-1)\n",
        "\n",
        "        position_in_expert_2 = position_in_expert_2.sum(dim=-1)\n",
        "        gate_2 *= mask_2_flat\n",
        "\n",
        "        # [batch, group, experts, expert_capacity]\n",
        "        combine_tensor = (\n",
        "            gate_1[..., None, None]\n",
        "            * mask_1_flat[..., None, None]\n",
        "            * F.one_hot(index_1, num_gates)[..., None]\n",
        "            * safe_one_hot(position_in_expert_1.long(), expert_capacity)[..., None, :] +\n",
        "            gate_2[..., None, None]\n",
        "            * mask_2_flat[..., None, None]\n",
        "            * F.one_hot(index_2, num_gates)[..., None]\n",
        "            * safe_one_hot(position_in_expert_2.long(), expert_capacity)[..., None, :]\n",
        "        )\n",
        "\n",
        "        dispatch_tensor = combine_tensor.bool().to(combine_tensor)\n",
        "        return dispatch_tensor, combine_tensor, loss\n",
        "\n",
        "# plain mixture of experts\n",
        "\n",
        "class MoE(nn.Module):\n",
        "    def __init__(self,\n",
        "        dim,\n",
        "        num_experts = 16,\n",
        "        hidden_dim = None,\n",
        "        activation = nn.ReLU,\n",
        "        second_policy_train = 'random',\n",
        "        second_policy_eval = 'random',\n",
        "        second_threshold_train = 0.2,\n",
        "        second_threshold_eval = 0.2,\n",
        "        capacity_factor_train = 1.25,\n",
        "        capacity_factor_eval = 2.,\n",
        "        loss_coef = 1e-2,\n",
        "        experts = None):\n",
        "        super().__init__()\n",
        "\n",
        "        self.num_experts = num_experts\n",
        "\n",
        "        gating_kwargs = {'second_policy_train': second_policy_train, 'second_policy_eval': second_policy_eval, 'second_threshold_train': second_threshold_train, 'second_threshold_eval': second_threshold_eval, 'capacity_factor_train': capacity_factor_train, 'capacity_factor_eval': capacity_factor_eval}\n",
        "        self.gate = Top2Gating(dim, num_gates = num_experts, **gating_kwargs)\n",
        "        self.experts = default(experts, lambda: Experts(dim, num_experts = num_experts, hidden_dim = hidden_dim, activation = activation))\n",
        "        self.loss_coef = loss_coef\n",
        "\n",
        "    def forward(self, inputs, **kwargs):\n",
        "        b, n, d, e = *inputs.shape, self.num_experts\n",
        "        dispatch_tensor, combine_tensor, loss = self.gate(inputs)\n",
        "        expert_inputs = torch.einsum('bnd,bnec->ebcd', inputs, dispatch_tensor)\n",
        "\n",
        "        # Now feed the expert inputs through the experts.\n",
        "        orig_shape = expert_inputs.shape\n",
        "        expert_inputs = expert_inputs.reshape(e, -1, d)\n",
        "        expert_outputs = self.experts(expert_inputs)\n",
        "        expert_outputs = expert_outputs.reshape(*orig_shape)\n",
        "\n",
        "        output = torch.einsum('ebcd,bnec->bnd', expert_outputs, combine_tensor)\n",
        "        return output, loss * self.loss_coef\n",
        "\n",
        "# 2-level heirarchical mixture of experts\n",
        "\n",
        "class HeirarchicalMoE(nn.Module):\n",
        "    def __init__(self,\n",
        "        dim,\n",
        "        num_experts = (4, 4),\n",
        "        hidden_dim = None,\n",
        "        activation = nn.ReLU,\n",
        "        second_policy_train = 'random',\n",
        "        second_policy_eval = 'random',\n",
        "        second_threshold_train = 0.2,\n",
        "        second_threshold_eval = 0.2,\n",
        "        capacity_factor_train = 1.25,\n",
        "        capacity_factor_eval = 2.,\n",
        "        loss_coef = 1e-2,\n",
        "        experts = None):\n",
        "        super().__init__()\n",
        "\n",
        "        assert len(num_experts) == 2, 'only 2 levels of heirarchy for experts allowed for now'\n",
        "        num_experts_outer, num_experts_inner = num_experts\n",
        "        self.num_experts_outer = num_experts_outer\n",
        "        self.num_experts_inner = num_experts_inner\n",
        "\n",
        "        gating_kwargs = {'second_policy_train': second_policy_train, 'second_policy_eval': second_policy_eval, 'second_threshold_train': second_threshold_train, 'second_threshold_eval': second_threshold_eval, 'capacity_factor_train': capacity_factor_train, 'capacity_factor_eval': capacity_factor_eval}\n",
        "\n",
        "        self.gate_outer = Top2Gating(dim, num_gates = num_experts_outer, **gating_kwargs)\n",
        "        self.gate_inner = Top2Gating(dim, num_gates = num_experts_inner, outer_expert_dims = (num_experts_outer,), **gating_kwargs)\n",
        "\n",
        "        self.experts = default(experts, lambda: Experts(dim, num_experts = num_experts, hidden_dim = hidden_dim, activation = activation))\n",
        "        self.loss_coef = loss_coef\n",
        "\n",
        "    def forward(self, inputs, **kwargs):\n",
        "        b, n, d, eo, ei = *inputs.shape, self.num_experts_outer, self.num_experts_inner\n",
        "        dispatch_tensor_outer, combine_tensor_outer, loss_outer = self.gate_outer(inputs)\n",
        "        expert_inputs_outer = torch.einsum('bnd,bnec->ebcd', inputs, dispatch_tensor_outer)\n",
        "\n",
        "        # we construct an \"importance\" Tensor for the inputs to the second-level\n",
        "        # gating.  The importance of an input is 1.0 if it represents the\n",
        "        # first-choice expert-group and 0.5 if it represents the second-choice expert\n",
        "        # group.  This is used by the second-level gating.\n",
        "        importance = combine_tensor_outer.permute(2, 0, 3, 1).sum(dim=-1)\n",
        "        importance = 0.5 * ((importance > 0.5).float() + (importance > 0.).float())\n",
        "\n",
        "        dispatch_tensor_inner, combine_tensor_inner, loss_inner = self.gate_inner(expert_inputs_outer, importance = importance)\n",
        "        expert_inputs = torch.einsum('ebnd,ebnfc->efbcd', expert_inputs_outer, dispatch_tensor_inner)\n",
        "\n",
        "        # Now feed the expert inputs through the experts.\n",
        "        orig_shape = expert_inputs.shape\n",
        "        expert_inputs = expert_inputs.reshape(eo, ei, -1, d)\n",
        "        expert_outputs = self.experts(expert_inputs)\n",
        "        expert_outputs = expert_outputs.reshape(*orig_shape)\n",
        "\n",
        "        # NOW COMBINE EXPERT OUTPUTS (reversing everything we have done)\n",
        "        # expert_output has shape [y0, x1, h, d, n]\n",
        "\n",
        "        expert_outputs_outer = torch.einsum('efbcd,ebnfc->ebnd', expert_outputs, combine_tensor_inner)\n",
        "        output = torch.einsum('ebcd,bnec->bnd', expert_outputs_outer, combine_tensor_outer)\n",
        "        return output, (loss_outer + loss_inner) * self.loss_coef"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4bC0xET8cv0b"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}